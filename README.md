# Awesome-Federated-LLM-Learning
This is a collection of research papers for Federated Learning for Large Language Models (FedLLM). And the repository will be continuously updated to track the frontier of FedLLM.


## Table of Contents
- [Published Papers](#item-1)
  - [2024](#item-11)
  - [2023](#item-12)
  - [2022](#item-13)
  - [2021](#item-14)
- [Preprints](#item-2)
  - [2024](#item-21)
  - [2023](#item-22)
  - [2022](#item-23)
- [Applications](#item-3)


<a id="item-1"></a>
## Published Papers

In this section, we will list recent FedLLM papers accepted by top tier AI/ML/Networking conferences.
```
format:
- [title](paper link) [Venue]
  - authors
  - datasets
  - models
  - [code](code link) [slide](slide link) 
```
<a id="item-11"></a>
### 2024

<a id="item-12"></a>
### 2023
- [Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization](http://arxiv.org/abs/2310.15080) [EMNLP 2023]
  - *authors*: Tianshi Che, Ji Liu, Yang Zhou, Jiaxiang Ren, Jiwen Zhou, Victor S. Sheng, Huaiyu Dai, Dejing Dou.
  - *datasets*: QNLI, SST-2, CoLA, MRPC, RTE, and BoolQ, MPQA, Subj, TREC, and MR 
  - *models*: RoBERTa, GPT2, LLaMA 7B
  - [code](https://github.com/llm-eff/FedPepTAO) and slide

- [FedPETuning: When Federated Learning Meets the Parameter-Efficient Tuning Methods of Pre-trained Language Models](https://aclanthology.org/2023.findings-acl.632/) [ACL 2023]
  - *authors*: Zhuo Zhang, Yuanhang Yang, Yong Dai, Qifan Wang, Yue Yu, Lizhen Qu, Zenglin Xu.
  - *datasets*: RTE, MRPC, SST-2, QNLI, QQP, MNLI 
  - *models*: RoBERTa
  - [code](https://github.com/SMILELab-FL/FedPETuning) and slide
 
<a id="item-13"></a>
### 2022


<a id="item-2"></a>
## Preprints

<a id="item-21"></a>
### 2024

<a id="item-22"></a>
### 2023

<a id="item-23"></a>
### 2022

<a id="item-3"></a>
## Applications
